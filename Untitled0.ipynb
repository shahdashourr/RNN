{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeGqG8eJSXQ6lMjPa6NkkH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahdashourr/RNN/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viSra1dqVoye",
        "outputId": "4242afe6-1783-4445-ca43-a3733189efd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: [8.78640757]\n",
            "Epoch 500, Loss: [1.54621203]\n",
            "Epoch 1000, Loss: [1.43841813]\n",
            "Epoch 1500, Loss: [0.05617766]\n",
            "Epoch 2000, Loss: [0.02529274]\n",
            "Epoch 2500, Loss: [0.01608301]\n",
            "Epoch 3000, Loss: [0.01171407]\n",
            "Epoch 3500, Loss: [0.00917845]\n",
            "Epoch 4000, Loss: [0.0075276]\n",
            "Epoch 4500, Loss: [0.00636954]\n",
            "Prediction for ['I', 'love', 'deep']: learning\n",
            "Prediction for ['machine', 'learning', 'is']: fun\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "data = [\n",
        "    (['I', 'love', 'deep'], 'learning'),\n",
        "    (['deep', 'learning', 'is'], 'awesome'),\n",
        "    (['I', 'enjoy', 'machine'], 'learning'),\n",
        "    (['machine', 'learning', 'is'], 'fun')\n",
        "]\n",
        "\n",
        "words = list(set([word for sentence, target in data for word in sentence] + [target for _, target in data]))\n",
        "word_to_idx = {word: idx for idx, word in enumerate(words)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "vocab_size = len(words)\n",
        "\n",
        "hidden_size = 16\n",
        "learning_rate = 0.01\n",
        "\n",
        "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
        "Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((vocab_size, 1))\n",
        "\n",
        "def word_to_one_hot(word):\n",
        "    one_hot = np.zeros((vocab_size, 1))\n",
        "    one_hot[word_to_idx[word]] = 1\n",
        "    return one_hot\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / np.sum(e_x)\n",
        "\n",
        "for epoch in range(5000):\n",
        "    total_loss = 0\n",
        "    for sentence, target in data:\n",
        "        hs = {}\n",
        "        hs[-1] = np.zeros((hidden_size, 1))\n",
        "\n",
        "        for t in range(len(sentence)):\n",
        "            xt = word_to_one_hot(sentence[t])\n",
        "            hs[t] = np.tanh(np.dot(Wxh, xt) + np.dot(Whh, hs[t-1]) + bh)\n",
        "\n",
        "        y = np.dot(Why, hs[len(sentence)-1]) + by\n",
        "        p = softmax(y)\n",
        "\n",
        "        target_idx = word_to_idx[target]\n",
        "        loss = -np.log(p[target_idx])\n",
        "        total_loss += loss\n",
        "\n",
        "        dWhy = np.zeros_like(Why)\n",
        "        dWxh = np.zeros_like(Wxh)\n",
        "        dWhh = np.zeros_like(Whh)\n",
        "        dbh = np.zeros_like(bh)\n",
        "        dby = np.zeros_like(by)\n",
        "        dhnext = np.zeros_like(hs[0])\n",
        "\n",
        "        dy = np.copy(p)\n",
        "        dy[target_idx] -= 1\n",
        "\n",
        "        dWhy += np.dot(dy, hs[len(sentence)-1].T)\n",
        "        dby += dy\n",
        "\n",
        "        for t in reversed(range(len(sentence))):\n",
        "            dh = np.dot(Why.T, dy) + dhnext\n",
        "            dhraw = (1 - hs[t] * hs[t]) * dh\n",
        "            dbh += dhraw\n",
        "            dWxh += np.dot(dhraw, word_to_one_hot(sentence[t]).T)\n",
        "            dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "            dhnext = np.dot(Whh.T, dhraw)\n",
        "\n",
        "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "            np.clip(dparam, -5, 5, out=dparam)\n",
        "\n",
        "        Wxh -= learning_rate * dWxh\n",
        "        Whh -= learning_rate * dWhh\n",
        "        Why -= learning_rate * dWhy\n",
        "        bh -= learning_rate * dbh\n",
        "        by -= learning_rate * dby\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {total_loss}')\n",
        "\n",
        "def predict(sentence):\n",
        "    hs = {}\n",
        "    hs[-1] = np.zeros((hidden_size, 1))\n",
        "    for t in range(len(sentence)):\n",
        "        xt = word_to_one_hot(sentence[t])\n",
        "        hs[t] = np.tanh(np.dot(Wxh, xt) + np.dot(Whh, hs[t-1]) + bh)\n",
        "\n",
        "    y = np.dot(Why, hs[len(sentence)-1]) + by\n",
        "    p = softmax(y)\n",
        "    idx = np.argmax(p)\n",
        "    return idx_to_word[idx]\n",
        "\n",
        "print(\"Prediction for ['I', 'love', 'deep']:\", predict(['I', 'love', 'deep']))\n",
        "print(\"Prediction for ['machine', 'learning', 'is']:\", predict(['machine', 'learning', 'is']))\n"
      ]
    }
  ]
}